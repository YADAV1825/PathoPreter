{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting download of dbNSFP4.9a.zip...\n",
      "This file is large (10GB+), so please wait. Do not close the tab.\n",
      "Downloaded: 1.01 GB...\n",
      "Downloaded: 2.00 GB...\n",
      "Downloaded: 3.01 GB...\n",
      "Downloaded: 4.00 GB...\n",
      "Downloaded: 5.00 GB...\n",
      "Downloaded: 6.01 GB...\n",
      "Downloaded: 7.00 GB...\n",
      "Downloaded: 8.01 GB...\n",
      "Downloaded: 9.00 GB...\n",
      "Downloaded: 10.00 GB...\n",
      "Downloaded: 11.01 GB...\n",
      "Downloaded: 12.00 GB...\n",
      "Downloaded: 13.01 GB...\n",
      "Downloaded: 14.00 GB...\n",
      "Downloaded: 15.00 GB...\n",
      "Downloaded: 16.01 GB...\n",
      "Downloaded: 17.00 GB...\n",
      "Downloaded: 18.01 GB...\n",
      "Downloaded: 19.00 GB...\n",
      "Downloaded: 20.00 GB...\n",
      "Downloaded: 21.01 GB...\n",
      "Downloaded: 22.00 GB...\n",
      "Downloaded: 23.01 GB...\n",
      "Downloaded: 24.00 GB...\n",
      "Downloaded: 25.00 GB...\n",
      "Downloaded: 26.01 GB...\n",
      "Downloaded: 27.00 GB...\n",
      "Downloaded: 28.01 GB...\n",
      "Downloaded: 29.00 GB...\n",
      "Downloaded: 30.00 GB...\n",
      "Downloaded: 31.01 GB...\n",
      "Downloaded: 32.00 GB...\n",
      "Downloaded: 33.01 GB...\n",
      "Downloaded: 34.00 GB...\n",
      "Downloaded: 35.00 GB...\n",
      "\n",
      "âœ… Download Complete! Saved as: dbNSFP4.9a.zip\n",
      "File size: 35.92 GB\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# The direct link you provided\n",
    "url = 'https://usf.box.com/shared/static/ohjs9p90r0fgt3u3pwt10o6m249i0j12'\n",
    "output_filename = 'dbNSFP4.9a.zip'\n",
    "\n",
    "print(f\"ðŸš€ Starting download of {output_filename}...\")\n",
    "print(\"This file is large (10GB+), so please wait. Do not close the tab.\")\n",
    "\n",
    "try:\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(output_filename, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            chunk_size = 1024 * 1024 * 10  # 10 MB chunks\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    # Print status every 1GB (approx)\n",
    "                    if downloaded % (1024 * 1024 * 1024) < chunk_size:\n",
    "                        print(f\"Downloaded: {downloaded / (1024*1024*1024):.2f} GB...\")\n",
    "                        \n",
    "    print(f\"\\nâœ… Download Complete! Saved as: {output_filename}\")\n",
    "    print(f\"File size: {os.path.getsize(output_filename) / (1024*1024*1024):.2f} GB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error downloading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  dbNSFP4.9a.zip\n",
      " extracting: dbnsfp_data/dbNSFP4.8a.readme.txt  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr1.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr10.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr11.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr12.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr13.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr14.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr15.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr16.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr17.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr18.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr19.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr2.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr20.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr21.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr22.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr3.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr4.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr5.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr6.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr7.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr8.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chr9.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chrM.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chrX.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8a_variant.chrY.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8_gene.complete.gz  \n",
      " extracting: dbnsfp_data/dbNSFP4.8_gene.gz  \n",
      " extracting: dbnsfp_data/LICENSE.txt  \n",
      " extracting: dbnsfp_data/search_dbNSFP48a.class  \n",
      " extracting: dbnsfp_data/search_dbNSFP48a.jar  \n",
      " extracting: dbnsfp_data/search_dbNSFP48a.readme.pdf  \n",
      " extracting: dbnsfp_data/try.vcf     \n",
      " extracting: dbnsfp_data/tryhg18.in  \n",
      " extracting: dbnsfp_data/tryhg19.in  \n",
      " extracting: dbnsfp_data/tryhg38.in  \n"
     ]
    }
   ],
   "source": [
    "!unzip dbNSFP4.9a.zip -d dbnsfp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (22.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ STARTING BENCHMARK EXTRACTION\n",
      "\n",
      "[1/4] Loading Test Set from test_unseen_variant_level.parquet...\n",
      "Loaded 55376 rows.\n",
      "Sample Extracted Name: ABCC9,\n",
      "\n",
      "[2/4] Loading ClinVar Summary (Mapping Name -> Chrom/Pos)...\n",
      "This can take 1-2 minutes...\n",
      "Found 38298 matching records in variant_summary.txt\n",
      "Successfully mapped 38298 variants to genomic coordinates.\n",
      "\n",
      "[3/4] Scanning dbNSFP database for CADD & REVEL scores...\n",
      "  > Scanning gz...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41208/3461606888.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ready['Chromosome'] = df_ready['Chromosome'].str.replace(\"chr\", \"\", case=False)\n",
      "/tmp/ipykernel_41208/3461606888.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ready['lookup_key'] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Scanning gz...\n",
      "  > Finished scanning.\n",
      "\n",
      "[4/4] Merging and Saving...\n",
      "\n",
      "âœ… SUCCESS! Results saved to: pathopreter_vs_sota.csv\n",
      "Matched scores for 1927 variants.\n",
      "Check the 'CADD_phred' and 'REVEL_score' columns in the output file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. Your LLM Test Set (The one with \"text\" prompt)\n",
    "TEST_SET_PATH = \"test_unseen_variant_level.parquet\"\n",
    "\n",
    "# 2. Your RAW ClinVar Data (The key to find coordinates)\n",
    "# Usually named 'variant_summary.txt'\n",
    "RAW_CLINVAR_PATH = \"data/variant_summary.txt\" \n",
    "\n",
    "# 3. dbNSFP Folder (Where you unzipped chr1, chr2, etc.)\n",
    "DBNSFP_FOLDER = \"dbnsfp_data\"\n",
    "\n",
    "# 4. Output Filename\n",
    "OUTPUT_FILE = \"pathopreter_vs_sota.csv\"\n",
    "# =================================================\n",
    "\n",
    "def extract_variant_nm(text):\n",
    "    \"\"\"\n",
    "    Extracts the NM_ identifier from the LLM prompt.\n",
    "    Example Input: \"Variant: NM_201384.3(PLEC):c.2406C>A...\"\n",
    "    Output: \"NM_201384.3(PLEC):c.2406C>A\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Regex to capture everything after \"Variant: \" until the next newline or space\n",
    "        match = re.search(r\"Variant:\\s+([^\\n\\s]+)\", text)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "print(\"ðŸš€ STARTING BENCHMARK EXTRACTION\")\n",
    "\n",
    "# --- STEP 1: Load Test Set & Extract Names ---\n",
    "print(f\"\\n[1/4] Loading Test Set from {TEST_SET_PATH}...\")\n",
    "df_test = pd.read_parquet(TEST_SET_PATH)\n",
    "\n",
    "# Extract the Name (NM_...) from the prompt text\n",
    "df_test['Name'] = df_test['text'].apply(extract_variant_nm)\n",
    "\n",
    "print(f\"Loaded {len(df_test)} rows.\")\n",
    "print(f\"Sample Extracted Name: {df_test['Name'].iloc[0]}\")\n",
    "\n",
    "# --- STEP 2: Load ClinVar Summary to get Coordinates ---\n",
    "print(f\"\\n[2/4] Loading ClinVar Summary (Mapping Name -> Chrom/Pos)...\")\n",
    "print(\"This can take 1-2 minutes...\")\n",
    "\n",
    "# We only need specific columns to map the name to coordinates\n",
    "# variant_summary.txt is tab-separated\n",
    "df_raw = pd.read_csv(\n",
    "    RAW_CLINVAR_PATH, \n",
    "    sep='\\t',\n",
    "    usecols=['Name', 'Chromosome', 'PositionVCF', 'ReferenceAlleleVCF', 'AlternateAlleleVCF'],\n",
    "    dtype={'Chromosome': str, 'PositionVCF': str}, # Keep as strings to avoid type errors\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Filter ClinVar to only include the variants in our test set (Optimization)\n",
    "# This drastically reduces the dataframe size for the merge\n",
    "test_names = set(df_test['Name'].unique())\n",
    "df_raw = df_raw[df_raw['Name'].isin(test_names)]\n",
    "\n",
    "print(f\"Found {len(df_raw)} matching records in variant_summary.txt\")\n",
    "\n",
    "# Merge coordinates into the test set\n",
    "df_mapped = df_test.merge(df_raw, on='Name', how='left')\n",
    "\n",
    "# Drop rows that failed to map (if any)\n",
    "df_ready = df_mapped.dropna(subset=['Chromosome', 'PositionVCF'])\n",
    "\n",
    "# Create a \"Lookup Key\" for dbNSFP: \"1:12345:A:G\"\n",
    "# Note: dbNSFP usually doesn't use \"chr\" prefix in the data columns, so we strip it if present\n",
    "df_ready['Chromosome'] = df_ready['Chromosome'].str.replace(\"chr\", \"\", case=False)\n",
    "\n",
    "df_ready['lookup_key'] = (\n",
    "    df_ready['Chromosome'] + \":\" + \n",
    "    df_ready['PositionVCF'] + \":\" + \n",
    "    df_ready['ReferenceAlleleVCF'] + \":\" + \n",
    "    df_ready['AlternateAlleleVCF']\n",
    ")\n",
    "\n",
    "# Create a set for O(1) instant lookup\n",
    "lookup_set = set(df_ready['lookup_key'])\n",
    "print(f\"Successfully mapped {len(df_ready)} variants to genomic coordinates.\")\n",
    "\n",
    "\n",
    "# --- STEP 3: Scan dbNSFP for Scores ---\n",
    "print(f\"\\n[3/4] Scanning dbNSFP database for CADD & REVEL scores...\")\n",
    "\n",
    "found_scores = []\n",
    "dbnsfp_files = glob.glob(os.path.join(DBNSFP_FOLDER, \"*variant.chr*\"))\n",
    "\n",
    "# Columns we want from dbNSFP\n",
    "# Note: Column names might vary slightly by version. \n",
    "# Standard 4.x names: '#chr', 'pos(1-based)', 'ref', 'alt', 'CADD_phred', 'REVEL_score'\n",
    "cols_to_use = ['#chr', 'pos(1-based)', 'ref', 'alt', 'CADD_phred', 'REVEL_score']\n",
    "\n",
    "for f_path in sorted(dbnsfp_files):\n",
    "    chr_name = os.path.basename(f_path).split('.')[-1] # e.g., chr1\n",
    "    print(f\"  > Scanning {chr_name}...\", end=\"\\r\")\n",
    "    \n",
    "    try:\n",
    "        # Read in chunks (100k rows) to save RAM\n",
    "        chunk_iter = pd.read_csv(\n",
    "            f_path, \n",
    "            sep='\\t', \n",
    "            usecols=lambda c: c in cols_to_use, # robust column loading\n",
    "            chunksize=100000, \n",
    "            low_memory=False,\n",
    "            dtype={'#chr': str, 'pos(1-based)': str}\n",
    "        )\n",
    "        \n",
    "        for chunk in chunk_iter:\n",
    "            # Create key\n",
    "            chunk['lookup_key'] = (\n",
    "                chunk['#chr'] + \":\" + \n",
    "                chunk['pos(1-based)'] + \":\" + \n",
    "                chunk['ref'] + \":\" + \n",
    "                chunk['alt']\n",
    "            )\n",
    "            \n",
    "            # Filter: Keep only rows that match our test set\n",
    "            matches = chunk[chunk['lookup_key'].isin(lookup_set)]\n",
    "            \n",
    "            if not matches.empty:\n",
    "                # Keep only key and scores\n",
    "                found_scores.append(matches[['lookup_key', 'CADD_phred', 'REVEL_score']])\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n    Skipping {chr_name}: {e}\")\n",
    "\n",
    "print(f\"\\n  > Finished scanning.\")\n",
    "\n",
    "# --- STEP 4: Merge & Save ---\n",
    "print(f\"\\n[4/4] Merging and Saving...\")\n",
    "\n",
    "if found_scores:\n",
    "    df_scores = pd.concat(found_scores)\n",
    "    \n",
    "    # Merge scores back into your main dataframe\n",
    "    final_df = df_ready.merge(df_scores, on='lookup_key', how='left')\n",
    "    \n",
    "    # Save\n",
    "    final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nâœ… SUCCESS! Results saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"Matched scores for {len(df_scores)} variants.\")\n",
    "    print(\"Check the 'CADD_phred' and 'REVEL_score' columns in the output file.\")\n",
    "else:\n",
    "    print(\"\\nâŒ CRITICAL: No matching scores found.\")\n",
    "    print(\"Check if dbNSFP files are unzipped correctly or if coordinate format matches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "      FINAL CORRECTED BENCHMARK\n",
      "========================================\n",
      "\n",
      "--- REVEL (Missense Variants Only) ---\n",
      "Sample Size (N): 243 (Pathogenic count: 18)\n",
      "REVEL Recall:    27.8%\n",
      "REVEL Accuracy:  84.4%\n",
      "\n",
      "--- CADD (All Variants) ---\n",
      "Sample Size (N): 1937 (Pathogenic count: 1592)\n",
      "CADD Recall:     99.0%\n",
      "CADD Accuracy:   94.7%\n",
      "\n",
      "--- YOUR PATHOPRETER ---\n",
      "Recall: ~94.0% (On full dataset)\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41208/3374293388.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['Pred'] = dataframe[score_col].apply(lambda x: \"Pathogenic\" if x >= threshold else \"Benign\")\n",
      "/tmp/ipykernel_41208/3374293388.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['Pred'] = dataframe[score_col].apply(lambda x: \"Pathogenic\" if x >= threshold else \"Benign\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your raw results\n",
    "df = pd.read_csv(\"pathopreter_vs_sota.csv\", low_memory=False)\n",
    "\n",
    "# 1. Cleaning Function (Improved)\n",
    "def get_max_score(value):\n",
    "    \"\"\"\n",
    "    Parses '0.5;0.6' or '.;0.5' strings.\n",
    "    Returns None if no valid number exists.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = str(value).split(';')\n",
    "        valid_nums = []\n",
    "        for x in parts:\n",
    "            x = x.strip()\n",
    "            # Check for real numbers, ignore dots/empty\n",
    "            if x != '.' and x != '' and x.lower() != 'nan':\n",
    "                try:\n",
    "                    valid_nums.append(float(x))\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if not valid_nums:\n",
    "            return None # Return None instead of 0.0 to distinguish \"Missing\" from \"Safe\"\n",
    "            \n",
    "        return max(valid_nums)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply cleaning\n",
    "df['CADD_Final'] = df['CADD_phred'].apply(get_max_score)\n",
    "df['REVEL_Final'] = df['REVEL_score'].apply(get_max_score)\n",
    "\n",
    "# 2. CREATE TWO SEPARATE DATASETS (Fair Comparison)\n",
    "# REVEL is picky, CADD is broad. We can't use the same N for both if we want to be fair.\n",
    "\n",
    "# --- DATASET A: The \"REVEL-Applicable\" Subset (Missense Only) ---\n",
    "df_revel = df.dropna(subset=['REVEL_Final'])\n",
    "\n",
    "# --- DATASET B: The \"CADD-Applicable\" Subset (Almost everything) ---\n",
    "df_cadd = df.dropna(subset=['CADD_Final'])\n",
    "\n",
    "# 3. Calculate Metrics\n",
    "def calculate_metrics(dataframe, score_col, threshold):\n",
    "    # Predict\n",
    "    dataframe['Pred'] = dataframe[score_col].apply(lambda x: \"Pathogenic\" if x >= threshold else \"Benign\")\n",
    "    \n",
    "    # Overall Accuracy\n",
    "    acc = (dataframe['Pred'] == dataframe['clean_label']).mean()\n",
    "    \n",
    "    # Recall (Sensitivity) - Only on Pathogenic rows\n",
    "    path_rows = dataframe[dataframe['clean_label'] == 'Pathogenic']\n",
    "    if len(path_rows) > 0:\n",
    "        recall = (path_rows['Pred'] == 'Pathogenic').mean()\n",
    "    else:\n",
    "        recall = 0.0\n",
    "        \n",
    "    return acc, recall, len(dataframe), len(path_rows)\n",
    "\n",
    "# Get Stats\n",
    "revel_acc, revel_recall, revel_n, revel_path_n = calculate_metrics(df_revel, 'REVEL_Final', 0.5)\n",
    "cadd_acc, cadd_recall, cadd_n, cadd_path_n = calculate_metrics(df_cadd, 'CADD_Final', 20.0)\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"      FINAL CORRECTED BENCHMARK\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f\"\\n--- REVEL (Missense Variants Only) ---\")\n",
    "print(f\"Sample Size (N): {revel_n} (Pathogenic count: {revel_path_n})\")\n",
    "print(f\"REVEL Recall:    {revel_recall*100:.1f}%\")\n",
    "print(f\"REVEL Accuracy:  {revel_acc*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n--- CADD (All Variants) ---\")\n",
    "print(f\"Sample Size (N): {cadd_n} (Pathogenic count: {cadd_path_n})\")\n",
    "print(f\"CADD Recall:     {cadd_recall*100:.1f}%\")\n",
    "print(f\"CADD Accuracy:   {cadd_acc*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n--- YOUR PATHOPRETER ---\")\n",
    "print(f\"Recall: ~94.0% (On full dataset)\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAW vs CLEANED Check ---\n",
      "Raw: .;0.788;0.788        -> Cleaned: 0.788 -> Prediction: Pathogenic\n",
      "Raw: 0.106;.              -> Cleaned: 0.106 -> Prediction: Benign\n",
      "Raw: 0.027                -> Cleaned: 0.027 -> Prediction: Benign\n",
      "Raw: 0.188;0.188          -> Cleaned: 0.188 -> Prediction: Benign\n",
      "Raw: 0.027;.;0.027;.;.    -> Cleaned: 0.027 -> Prediction: Benign\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: See exactly what the cleaner did\n",
    "print(\"--- RAW vs CLEANED Check ---\")\n",
    "for i, row in df_revel.head(5).iterrows():\n",
    "    raw = row['REVEL_score']\n",
    "    clean = row['REVEL_Final']\n",
    "    pred = row['Pred']\n",
    "    print(f\"Raw: {str(raw):<20} -> Cleaned: {clean:<5} -> Prediction: {pred}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
