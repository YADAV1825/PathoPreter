{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (8.1.1)\n",
      "Requirement already satisfied: widgetsnbextension in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.0.15)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipywidgets) (8.17.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack-data in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "‚úÖ A100 Environment Ready: TF32 Enabled & System Patched.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install PyTorch with A100 CUDA support\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu121 --quiet\n",
    "\n",
    "# 2. Install Unsloth & Core Libraries\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes psutil ipywidgets --quiet\n",
    "!pip install pandas pyarrow fastparquet --quiet\n",
    "!pip install ipywidgets widgetsnbextension\n",
    "# 3. CRITICAL FIX: Patch 'psutil' for Python 3.12\n",
    "import builtins\n",
    "import psutil\n",
    "builtins.psutil = psutil\n",
    "\n",
    "# 4. Enable A100 Math Acceleration (TF32)\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(\"‚úÖ A100 Environment Ready: TF32 Enabled & System Patched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting pipeline: Rohit_Pathopredict_A100_Redline\n",
      "   [Timestamp] 2025-12-24 08:14:16\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class ProLogger:\n",
    "    def __init__(self, project_name):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"\\nüöÄ Starting pipeline: {project_name}\")\n",
    "        print(f\"   [Timestamp] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    def log_config(self, config_dict):\n",
    "        print(f\"[config] Configuration loaded:\")\n",
    "        for key, value in config_dict.items():\n",
    "            print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "    \n",
    "    def log_hardware(self):\n",
    "        gpu_stats = torch.cuda.get_device_properties(0)\n",
    "        vram = round(gpu_stats.total_memory / 1024**3, 2)\n",
    "        print(f\"[hardware] Detected Device: {gpu_stats.name}\")\n",
    "        print(f\"[hardware] VRAM Available:  {vram} GB\")\n",
    "        if vram > 35:\n",
    "            print(f\"[hardware] Status: üü¢ A100 High-Bandwidth Mode Active\")\n",
    "        else:\n",
    "            print(f\"[hardware] Status: üü° Standard Mode\")\n",
    "\n",
    "    def log_step(self, tag, message):\n",
    "        print(f\"[{tag}] {message}\")\n",
    "\n",
    "    def log_success(self, message):\n",
    "        elapsed = round((time.time() - self.start_time) / 60, 2)\n",
    "        print(f\"‚úÖ {message} (Total Runtime: {elapsed} min)\")\n",
    "\n",
    "# Initialize\n",
    "logger = ProLogger(\"Rohit_Pathopredict_A100_Redline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "[config] Configuration loaded:\n",
      "   ‚Ä¢ model_name: Qwen/Qwen3-4B-Instruct-2507\n",
      "   ‚Ä¢ max_seq_length: 2048\n",
      "   ‚Ä¢ load_in_4bit: True\n",
      "   ‚Ä¢ lora_rank: 16\n",
      "   ‚Ä¢ target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "[hardware] Detected Device: NVIDIA A100-SXM4-40GB\n",
      "[hardware] VRAM Available:  39.49 GB\n",
      "[hardware] Status: üü¢ A100 High-Bandwidth Mode Active\n",
      "[model] Loading base model: Qwen/Qwen3-4B-Instruct-2507...\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.495 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model] Injecting LoRA adapters (Dropout=0.05)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.12.9 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] Loading full dataset...\n",
      "[data] Cutting dataset to 70% size...\n",
      "[data] Original Size: 2,217,246\n",
      "[data] Training Size (70%): 1,097,536\n",
      "[data] Validation Size (1%): 11,087 (For Early Stopping)\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Configuration\n",
    "config = {\n",
    "    \"model_name\": \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"lora_rank\": 16,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "}\n",
    "\n",
    "logger.log_config(config)\n",
    "logger.log_hardware()\n",
    "\n",
    "# 2. Load Base Model\n",
    "logger.log_step(\"model\", f\"Loading base model: {config['model_name']}...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = config['model_name'],\n",
    "    max_seq_length = config['max_seq_length'],\n",
    "    dtype = None, \n",
    "    load_in_4bit = config['load_in_4bit'],\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "# 3. Attach LoRA (With Safety Dropout)\n",
    "logger.log_step(\"model\", \"Injecting LoRA adapters (Dropout=0.05)...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = config['lora_rank'],\n",
    "    target_modules = config['target_modules'],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05, # Safety: Prevents over-memorization\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# 4. Load & Slice Data\n",
    "logger.log_step(\"data\", \"Loading full dataset...\")\n",
    "full_dataset = load_dataset(\"parquet\", data_files={\"train\": \"clinvar_llm_train.parquet\"})[\"train\"]\n",
    "\n",
    "# --- ‚úÇÔ∏è THE 70% CUT ---\n",
    "logger.log_step(\"data\", \"Cutting dataset to 70% size...\")\n",
    "full_dataset = full_dataset.shuffle(seed=3407) # Shuffle first to avoid bias\n",
    "keep_count = int(len(full_dataset) * 0.50)\n",
    "subset_dataset = full_dataset.select(range(keep_count))\n",
    "\n",
    "# --- üõ°Ô∏è VALIDATION SPLIT (1%) ---\n",
    "# We take 1% of the REMAINING 70% to use for Early Stopping tests\n",
    "split_dataset = subset_dataset.train_test_split(test_size=0.01)\n",
    "\n",
    "logger.log_step(\"data\", f\"Original Size: {len(full_dataset):,}\")\n",
    "logger.log_step(\"data\", f\"Training Size (70%): {len(split_dataset['train']):,}\")\n",
    "logger.log_step(\"data\", f\"Validation Size (1%): {len(split_dataset['test']):,} (For Early Stopping)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training] Configuring Parameters (Batch 160 | Save every 1k)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17396947a454a9baf0f870af7a4c953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=34):   0%|          | 0/1097536 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a39fee834a47a286dd62950494c822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=34):   0%|          | 0/11087 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training] üî• STARTING TRAINING (Checkpoints @ 1000 steps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,097,536 | Num Epochs = 1 | Total steps = 6,236\n",
      "O^O/ \\_/ \\    Batch size per device = 176 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (176 x 1 x 1) = 176\n",
      " \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6236' max='6236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6236/6236 4:58:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.409600</td>\n",
       "      <td>0.390952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.383700</td>\n",
       "      <td>0.347993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.332097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.372300</td>\n",
       "      <td>0.324486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>0.318403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>0.314631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training Pipeline Complete. (Total Runtime: 300.93 min)\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "import torch\n",
    "\n",
    "# Force TF32 for Speed\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "logger.log_step(\"training\", \"Configuring Parameters (Batch 176 | Save every 1k)...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"rohit_pathopredict_checkpoints\",\n",
    "\n",
    "    # üöÄ BATCH SIZE 160 (Safe from OOM)\n",
    "    per_device_train_batch_size = 176, \n",
    "    gradient_accumulation_steps = 1,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    \n",
    "    # üõ°Ô∏è CHECKPOINT STRATEGY (User Request: Every 1000 steps)\n",
    "    eval_strategy = \"steps\",      \n",
    "    eval_steps = 1000,            # Only check accuracy every 1000 steps\n",
    "    save_strategy = \"steps\",      \n",
    "    save_steps = 1000,            # Only save to disk every 1000 steps\n",
    "    save_total_limit = 2,         # Keep only the 2 best checkpoints to save space\n",
    "    \n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    \n",
    "    # OPTIMIZER & PRECISION\n",
    "    optim = \"adamw_8bit\",\n",
    "    bf16 = True,\n",
    "    \n",
    "    # MAX DATA LOADING\n",
    "    dataloader_num_workers = 16,\n",
    "    dataloader_pin_memory = True,\n",
    "    \n",
    "    # SCHEDULER\n",
    "    learning_rate = 2e-4,\n",
    "    warmup_steps = 100,\n",
    "    max_steps = -1,\n",
    "    num_train_epochs = 1,\n",
    "    \n",
    "    # LOGGING\n",
    "    logging_steps = 10,  # Still shows the progress bar update often\n",
    "    report_to = \"none\",\n",
    "    seed = 3407,\n",
    "    \n",
    "    # SPEED OPTIMIZATIONS\n",
    "    gradient_checkpointing = False,\n",
    "    group_by_length = True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = split_dataset[\"train\"],\n",
    "    eval_dataset = split_dataset[\"test\"], \n",
    "    \n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = config['max_seq_length'],\n",
    "    dataset_num_proc = 12,\n",
    "    \n",
    "    # Unsloth packing handling\n",
    "    packing = False, \n",
    "    \n",
    "    args = training_args,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "logger.log_step(\"training\", \"üî• STARTING TRAINING (Checkpoints @ 1000 steps)...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "logger.log_success(\"Training Pipeline Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[io] Saving best model to: final_rohit_pathopredict_qwen3\n",
      "‚úÖ Model saved successfully. Ready for validation. (Total Runtime: 300.95 min)\n"
     ]
    }
   ],
   "source": [
    "output_path = \"final_rohit_pathopredict_qwen3\"\n",
    "\n",
    "logger.log_step(\"io\", f\"Saving best model to: {output_path}\")\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "\n",
    "logger.log_success(f\"Model saved successfully. Ready for validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# CELL 7: EXPORT TO GGUF (For Ollama / LMStudio)\n",
    "# --------------------------------------------------------------------------------\n",
    "logger.log_step(\"export\", \"Starting GGUF Export Process...\")\n",
    "\n",
    "# 1. Define Export Methods\n",
    "# \"q4_k_m\" = Standard 4-bit (Fast, Low RAM, Good Quality) - Best for Ollama\n",
    "# \"q8_0\"   = 8-bit (High Precision, slow) - Best for archiving\n",
    "quantization_methods = [\"q4_k_m\", \"q8_0\"]\n",
    "\n",
    "for method in quantization_methods:\n",
    "    save_filename = f\"rohit_pathopredict_qwen3_{method}\"\n",
    "    \n",
    "    logger.log_step(\"export\", f\"Converting to {method.upper()} format...\")\n",
    "    try:\n",
    "        model.save_pretrained_gguf(\n",
    "            save_filename,\n",
    "            tokenizer,\n",
    "            quantization_method = method,\n",
    "        )\n",
    "        logger.log_success(f\"Exported: {save_filename}.gguf\")\n",
    "        \n",
    "        # 2. Create Ollama Modelfile (Auto-Generated)\n",
    "        # This lets you run 'ollama create rohit_model -f Modelfile' instantly\n",
    "        if method == \"q4_k_m\":\n",
    "            with open(f\"{save_filename}/Modelfile\", \"w\") as f:\n",
    "                f.write(f\"FROM ./{save_filename}.gguf\\n\")\n",
    "                f.write(\"TEMPLATE \\\"{{ .System }}\\nUser: {{ .Prompt }}\\nAssistant: \\\"\\n\")\n",
    "                f.write(\"SYSTEM \\\"You are an expert genetic variant classifier. Classify variants as Pathogenic, Benign, or Uncertain.\\\"\\n\")\n",
    "                f.write(\"PARAMETER temperature 0.1\\n\")\n",
    "                f.write(\"PARAMETER num_ctx 4096\\n\")\n",
    "            print(f\"   üìÑ Created Ollama Modelfile at: {save_filename}/Modelfile\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to export {method}: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ ALL SYSTEMS GO! PIPELINE FINISHED.\")\n",
    "print(\"To use in Ollama: cd rohit_pathopredict_qwen3_q4_k_m && ollama create rohit_patho -f Modelfile\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
