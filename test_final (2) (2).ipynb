{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ A100 Environment Ready: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# 1. Install Unsloth with H100 Support\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
    "\n",
    "# 2. Install Core Libraries\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes psutil pandas pyarrow fastparquet scikit-learn --quiet\n",
    "\n",
    "# 3. CRITICAL: Python 3.12 Patch\n",
    "import builtins\n",
    "import psutil\n",
    "builtins.psutil = psutil\n",
    "\n",
    "# 4. Enable H100 Specific Math (TF32 & BF16)\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(f\"‚úÖ A100 Environment Ready: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "üîÑ Loading model from: final_rohit_pathopredict_qwen3...\n",
      "==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.495 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.12.9 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ A100 Model Loaded & Optimized!\n",
      "\n",
      "============================================================\n",
      "üöÄ STARTING A100 ABLATION SUITE (TF32 Enabled)\n",
      "============================================================\n",
      "\n",
      "üìä DATASET: test_unseen_variant_level\n",
      "----------------------------------------\n",
      "   Rows: 55,376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Processing:   0%|          | 0/433 [00:00<?, ?batch/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth/kernels/utils.py:963: UserWarning: An output with one or more elements was resized since it had shape [1, 128, 2560], which does not match the required output shape [128, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth/kernels/utils.py:970: UserWarning: An output with one or more elements was resized since it had shape [1, 128, 2560], which does not match the required output shape [128, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W, out = out)\n",
      "   Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 432/433 [17:58<00:03,  3.25s/batch]/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth/kernels/utils.py:963: UserWarning: An output with one or more elements was resized since it had shape [1, 80, 2560], which does not match the required output shape [80, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth/kernels/utils.py:970: UserWarning: An output with one or more elements was resized since it had shape [1, 80, 2560], which does not match the required output shape [80, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W, out = out)\n",
      "   Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 433/433 [18:00<00:00,  2.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  pathogenic       0.95      0.94      0.94      7071\n",
      "      benign       0.99      0.99      0.99     48305\n",
      "   uncertain       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     55376\n",
      "   macro avg       0.65      0.64      0.65     55376\n",
      "weighted avg       0.99      0.99      0.99     55376\n",
      "\n",
      "üìâ Confusion Matrix:\n",
      "               Pathogenic   Benign   Uncertain\n",
      "  Pathogenic   6644         427      0\n",
      "  Benign       365          47940    0\n",
      "  Uncertain    0            0        0\n",
      "\n",
      "============================================================\n",
      "üèÅ DONE.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è A100 CONFIGURATION\n",
    "# ==========================================\n",
    "MODEL_PATH = \"final_rohit_pathopredict_qwen3\"  \n",
    "BATCH_SIZE = 128    \n",
    "\n",
    "TEST_FILES = [\n",
    "    \"test_unseen_variant_level.parquet\"\n",
    "]\n",
    "# ==========================================\n",
    "\n",
    "# ‚ö° OPTIMIZATION: Enable TF32 Math (A100 Speed Secret)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# 1. Load Model\n",
    "print(f\"üîÑ Loading model from: {MODEL_PATH}...\")\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"‚ùå CRITICAL: Folder '{MODEL_PATH}' not found.\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_PATH,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, \n",
    ")\n",
    "\n",
    "# Enable A100 Native Fast Inference\n",
    "FastLanguageModel.for_inference(model) \n",
    "tokenizer.padding_side = \"left\" # ‚ö° Faster generation\n",
    "\n",
    "print(\"‚úÖ A100 Model Loaded & Optimized!\")\n",
    "\n",
    "# 2. Speed-Optimized Predictor\n",
    "def get_batch_predictions(prompts, batch_size):\n",
    "    predictions = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"   Processing\", unit=\"batch\"):\n",
    "        batch_prompts = prompts[i : i + batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=2048\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=10, \n",
    "                use_cache=True, \n",
    "                temperature=0.01,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        predictions.extend([p.lower().strip() for p in decoded])\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# 3. Run The Suite\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STARTING A100 ABLATION SUITE (TF32 Enabled)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for file_path in TEST_FILES:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ö†Ô∏è  Skipping: {file_path}\")\n",
    "        continue\n",
    "        \n",
    "    dataset_name = os.path.basename(file_path).replace(\".parquet\", \"\")\n",
    "    print(f\"\\nüìä DATASET: {dataset_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        print(f\"   Rows: {len(df):,}\")\n",
    "        \n",
    "        prompts = [\n",
    "            row['text'].split(\"### Response:\")[0] + \"### Response:\\n\" \n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # üî• RUN INFERENCE\n",
    "        raw_preds = get_batch_predictions(prompts, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # Scoring\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for truth, pred in zip(df['clean_label'], raw_preds):\n",
    "            t_clean = truth.lower()\n",
    "            p_clean = pred\n",
    "            \n",
    "            true_label = \"other\"\n",
    "            pred_label = \"other\"\n",
    "            \n",
    "            if \"pathogenic\" in t_clean: true_label = \"pathogenic\"\n",
    "            elif \"benign\" in t_clean:   true_label = \"benign\"\n",
    "            elif \"uncertain\" in t_clean: true_label = \"uncertain\"\n",
    "                \n",
    "            if \"pathogenic\" in p_clean: pred_label = \"pathogenic\"\n",
    "            elif \"benign\" in p_clean:   pred_label = \"benign\"\n",
    "            elif \"uncertain\" in p_clean: pred_label = \"uncertain\"\n",
    "            \n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(pred_label)\n",
    "\n",
    "        # Report\n",
    "        labels = [\"pathogenic\", \"benign\", \"uncertain\"]\n",
    "        print(\"\\nüîç Classification Report:\")\n",
    "        print(classification_report(y_true, y_pred, labels=labels, zero_division=0))\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        print(\"üìâ Confusion Matrix:\")\n",
    "        print(f\"               Pathogenic   Benign   Uncertain\")\n",
    "        print(f\"  Pathogenic   {cm[0][0]:<12} {cm[0][1]:<8} {cm[0][2]}\")\n",
    "        print(f\"  Benign       {cm[1][0]:<12} {cm[1][1]:<8} {cm[1][2]}\")\n",
    "        print(f\"  Uncertain    {cm[2][0]:<12} {cm[2][1]:<8} {cm[2][2]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÅ DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Starting GGUF Export Process...\n",
      "   ‚Ü≥ Converting to Q4_K_M format...\n",
      "Unsloth: Merging model weights to 16-bit format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7063a9049fe454faf634a08f52a5966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /teamspace/studios/this_studio/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c4690d45bf4f86836e66c7ae99d4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29be06e12cff48a7a01cae98072a0230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.32s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76afaa78e15d473a8dff4e8db4b6f40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:21<00:00, 10.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/teamspace/studios/this_studio/rohit_pathopredict_qwen3_q4_k_m`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF bf16 to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: Updating system package directories\n",
      "Unsloth: Missing packages: libcurl4-openssl-dev\n",
      "Unsloth: Will attempt to install missing system packages.\n",
      "Unsloth: Installing packages: libcurl4-openssl-dev\n",
      "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
      "Unsloth: Cloning llama.cpp repository\n",
      "Unsloth: Install GGUF and other packages\n",
      "Unsloth: Successfully installed llama.cpp!\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into bf16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['qwen3-4b-instruct-2507.BF16.gguf']\n",
      "Unsloth: [2] Converting GGUF bf16 into q4_k_m. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['qwen3-4b-instruct-2507.Q4_K_M.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model qwen3-4b-instruct-2507.Q4_K_M.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n",
      "   ‚úÖ Exported: rohit_pathopredict_qwen3_q4_k_m.gguf\n",
      "      üìÑ Created Ollama Modelfile at: rohit_pathopredict_qwen3_q4_k_m/Modelfile\n",
      "   ‚Ü≥ Converting to Q8_0 format...\n",
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /teamspace/studios/this_studio/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21405a496edb47ed8cd052e6c55415ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfc8dc2cf6e424380aa63c2d9bc2f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.54s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f068db7cd94f44bbeb706be20aa190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:20<00:00, 10.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/teamspace/studios/this_studio/rohit_pathopredict_qwen3_q8_0`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF bf16 to ['q8_0'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: llama.cpp found in the system. Skipping installation.\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into bf16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['qwen3-4b-instruct-2507.BF16.gguf']\n",
      "Unsloth: [2] Converting GGUF bf16 into q8_0. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['qwen3-4b-instruct-2507.Q8_0.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model qwen3-4b-instruct-2507.Q8_0.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n",
      "   ‚úÖ Exported: rohit_pathopredict_qwen3_q8_0.gguf\n",
      "\n",
      "============================================================\n",
      "üöÄ PIPELINE FINISHED.\n",
      "To use in Ollama: cd rohit_pathopredict_qwen3_q4_k_m && ollama create rohit_patho -f Modelfile\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# CELL 2: EXPORT TO GGUF\n",
    "# --------------------------------------------------------------------------------\n",
    "print(\"üì¶ Starting GGUF Export Process...\")\n",
    "\n",
    "quantization_methods = [\"q4_k_m\", \"q8_0\"]\n",
    "\n",
    "for method in quantization_methods:\n",
    "    save_filename = f\"rohit_pathopredict_qwen3_{method}\"\n",
    "    \n",
    "    print(f\"   ‚Ü≥ Converting to {method.upper()} format...\")\n",
    "    try:\n",
    "        model.save_pretrained_gguf(\n",
    "            save_filename,\n",
    "            tokenizer,\n",
    "            quantization_method = method,\n",
    "        )\n",
    "        print(f\"   ‚úÖ Exported: {save_filename}.gguf\")\n",
    "        \n",
    "        if method == \"q4_k_m\":\n",
    "            with open(f\"{save_filename}/Modelfile\", \"w\") as f:\n",
    "                f.write(f\"FROM ./{save_filename}.gguf\\n\")\n",
    "                f.write(\"TEMPLATE \\\"{{ .System }}\\nUser: {{ .Prompt }}\\nAssistant: \\\"\\n\")\n",
    "                f.write(\"SYSTEM \\\"You are an expert genetic variant classifier. Classify variants as Pathogenic, Benign, or Uncertain.\\\"\\n\")\n",
    "                f.write(\"PARAMETER temperature 0.1\\n\")\n",
    "                f.write(\"PARAMETER num_ctx 4096\\n\")\n",
    "            print(f\"      üìÑ Created Ollama Modelfile at: {save_filename}/Modelfile\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed to export {method}: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ PIPELINE FINISHED.\")\n",
    "print(\"To use in Ollama: cd rohit_pathopredict_qwen3_q4_k_m && ollama create rohit_patho -f Modelfile\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è Loading RAW baseline model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f6a1f8c83f48719bb6e15dd504cedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d748c63d0174c1d868bb42145b9263f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc72b82689048aba280d099d768c2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ebdd6d8b25490892764b3e26aa898d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70067fb8b7c408f8cdfbe0c6eb3c81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a673e4d79b2f4900bc4f998e94f2a0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57a7751e09e421e99cba01d1f4c71df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df33677c1474dfc8c680ea3ebf1a722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b50309a25e641ae974be406442a3e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b684f8e531a417da69d4d8cb3414148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21943e02e7bd4378aa20266e428f75a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab84d6b3e44241dc8bbaf27ee9218274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb7f5cbf9f74f2da9d77469085a19e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Raw model loaded.\n",
      "Test size: 55376\n",
      "Columns found: ['text', 'clean_label', 'variant_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw model thinking: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3461/3461 [24:07<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä RAW MODEL BASELINE RESULTS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.87      1.00      0.93     48305\n",
      "  pathogenic       0.53      0.00      0.01      7071\n",
      "\n",
      "    accuracy                           0.87     55376\n",
      "   macro avg       0.70      0.50      0.47     55376\n",
      "weighted avg       0.83      0.87      0.81     55376\n",
      "\n",
      "üìâ Confusion Matrix\n",
      "[[   30  7041]\n",
      " [   27 48278]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è CONFIGURATION\n",
    "# ==========================================\n",
    "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # Double check this ID! (Qwen3 isn't out publicly yet, assumed 2.5)\n",
    "# If you have a specific private repo, keep your ID.\n",
    "TEST_FILE = \"test_unseen_variant_level.parquet\"\n",
    "BATCH_SIZE = 16   \n",
    "\n",
    "print(f\"‚¨áÔ∏è Loading RAW baseline model: {MODEL_ID}\")\n",
    "\n",
    "# Load Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(\"‚úÖ Raw model loaded.\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Prediction function\n",
    "# -----------------------------------------\n",
    "def predict(prompts):\n",
    "    preds = []\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Raw model thinking\"):\n",
    "        batch = prompts[i:i+BATCH_SIZE]\n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=2048\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=10, \n",
    "                temperature=0.01,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(\n",
    "            out[:, inputs.input_ids.shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        preds.extend([d.lower().strip() for d in decoded])\n",
    "        \n",
    "    return preds\n",
    "\n",
    "# -----------------------------------------\n",
    "# Run\n",
    "# -----------------------------------------\n",
    "df = pd.read_parquet(TEST_FILE)\n",
    "print(f\"Test size: {len(df)}\")\n",
    "\n",
    "# Check what columns we actually have\n",
    "print(f\"Columns found: {df.columns.tolist()}\")\n",
    "\n",
    "prompts = []\n",
    "for text in df[\"text\"]:\n",
    "    # We strip the answer part if it exists in the text column\n",
    "    input_text = text.split(\"### Response:\")[0] + \"\\n### Response:\\n\"\n",
    "    prompts.append(input_text)\n",
    "\n",
    "raw_preds = predict(prompts)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Scoring (FIXED HERE) üîß\n",
    "# -----------------------------------------\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "# Use 'clean_label' instead of 'label'\n",
    "target_col = 'clean_label' if 'clean_label' in df.columns else 'label'\n",
    "\n",
    "for truth, pred in zip(df[target_col], raw_preds):\n",
    "    t = str(truth).lower()\n",
    "    p = str(pred).lower()\n",
    "    \n",
    "    # Truth mapping\n",
    "    if \"pathogenic\" in t: true_val = \"pathogenic\"\n",
    "    elif \"benign\" in t:   true_val = \"benign\"\n",
    "    else: continue # Skip uncertains if any crept in\n",
    "\n",
    "    # Prediction mapping\n",
    "    if \"pathogenic\" in p: pred_val = \"pathogenic\"\n",
    "    elif \"benign\" in p:   pred_val = \"benign\"\n",
    "    else: pred_val = \"benign\" # Default to benign if model hallucinates or outputs gibberish\n",
    "\n",
    "    y_true.append(true_val)\n",
    "    y_pred.append(pred_val)\n",
    "\n",
    "print(\"\\nüìä RAW MODEL BASELINE RESULTS\")\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "print(\"üìâ Confusion Matrix\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=[\"pathogenic\", \"benign\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def free_vram(model=None, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Forcefully clears GPU memory. \n",
    "    If you pass 'model' and 'tokenizer', it deletes them first.\n",
    "    \"\"\"\n",
    "    print(\"üßπ Starting VRAM Cleanup...\")\n",
    "    \n",
    "    # 1. Delete Python objects if provided\n",
    "    if model:\n",
    "        del model\n",
    "    if tokenizer:\n",
    "        del tokenizer\n",
    "        \n",
    "    # 2. Garbage Collect (Python side)\n",
    "    gc.collect()\n",
    "    \n",
    "    # 3. Clear CUDA Cache (GPU side)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    # 4. Verification\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"‚úÖ VRAM Cleared.\")\n",
    "        print(f\"   - Memory Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"   - Memory Reserved:  {reserved:.2f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CUDA not available.\")\n",
    "\n",
    "# Usage Example:\n",
    "# free_vram(model, tokenizer) \n",
    "# OR just:\n",
    "# free_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ File: clinvar_evo2_labeled_HYBRID_200.parquet\n",
      "üìä Total Rows: 200\n",
      "\n",
      "üßê Column Names:\n",
      "['original_index', 'evo_score', 'pseudo_label', 'fetch_status', '#AlleleID', 'Type', 'Name', 'GeneID', 'GeneSymbol', 'HGNC_ID', 'ClinicalSignificance', 'ClinSigSimple', 'LastEvaluated', 'RS# (dbSNP)', 'nsv/esv (dbVar)', 'RCVaccession', 'PhenotypeIDS', 'PhenotypeList', 'Origin', 'OriginSimple', 'Assembly', 'ChromosomeAccession', 'Chromosome', 'Start', 'Stop', 'ReferenceAllele', 'AlternateAllele', 'Cytogenetic', 'ReviewStatus', 'NumberSubmitters', 'Guidelines', 'TestedInGTR', 'OtherIDs', 'SubmitterCategories', 'VariationID', 'PositionVCF', 'ReferenceAlleleVCF', 'AlternateAlleleVCF', 'SomaticClinicalImpact', 'SomaticClinicalImpactLastEvaluated', 'ReviewStatusClinicalImpact', 'Oncogenicity', 'OncogenicityLastEvaluated', 'ReviewStatusOncogenicity', 'SCVsForAggregateGermlineClassification', 'SCVsForAggregateSomaticClinicalImpact', 'SCVsForAggregateOncogenicityClassification']\n",
      "\n",
      "üè∑Ô∏è Label Counts (pseudo_label):\n",
      "pseudo_label\n",
      "ModelError                  198\n",
      "Ref_Mismatch_Both_Builds      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üëÄ First 5 rows:\n",
      "  GeneSymbol pseudo_label evo_score\n",
      "0     ZNF592   ModelError      None\n",
      "1     ZNF592   ModelError      None\n",
      "2      NUBPL   ModelError      None\n",
      "3      NUBPL   ModelError      None\n",
      "4        HFE   ModelError      None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"clinvar_evo2_labeled_HYBRID_200.parquet\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(f\"üìÇ File: {file_path}\")\n",
    "    print(f\"üìä Total Rows: {len(df)}\")\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        print(\"\\nüßê Column Names:\")\n",
    "        print(df.columns.tolist())\n",
    "        \n",
    "        print(\"\\nüè∑Ô∏è Label Counts (pseudo_label):\")\n",
    "        print(df['pseudo_label'].value_counts())\n",
    "        \n",
    "        print(\"\\nüëÄ First 5 rows:\")\n",
    "        print(df[['GeneSymbol', 'pseudo_label', 'evo_score']].head())\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è The file is empty! Evo2 didn't confidently label anything.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
